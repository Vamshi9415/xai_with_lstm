{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039c19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40d95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and clean\n",
    "\n",
    "def load_and_clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[['Category','Message']]\n",
    "    df.index = range( df.shape[0])\n",
    "    le = LabelEncoder()\n",
    "    df['Category_encoded'] = le.fit_transform(df['Category'])\n",
    "    df = df.drop(columns=['Category'])\n",
    "    return df\n",
    "\n",
    "def tokenize_and_pad(df,max_vocab_ratio=0.15,max_length = 100):\n",
    "    total_word_count = df['Message'].apply(lambda x : len(x.split(' '))).sum()\n",
    "    max_vocab_size = math.floor(total_word_count*max_vocab_ratio)\n",
    "    tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<oov>\" , filters='!@#$%^&*()_+{;}|:\"<>?', lower=True)\n",
    "    tokenizer.fit_on_texts(df['Message'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['Message'])\n",
    "    X = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    y = df['Category_encoded'].values\n",
    "    return X, y, tokenizer, max_vocab_size\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
    "import tensorflow as tf\n",
    "def build_lstm_model_1_layer(vocabulary_size,max_vocab_size,embedding_dim,max_length,lstm_units):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim = min(vocabulary_size,max_vocab_size),\n",
    "                    output_dim = embedding_dim,\n",
    "                    input_length = max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        LSTM(lstm_units,\n",
    "               dropout = 0.2,\n",
    "               recurrent_dropout=0.2,\n",
    "               return_sequences=True),\n",
    "        Dense(16,activation='relu'),\n",
    "        Dense(1,activation='sigmoid')    \n",
    "    ])\n",
    "    model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    tf.keras.backend.clear_session()\n",
    "    model.build(input_shape=(None, max_length))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_lstm_model_2_layer(vocabulary_size,max_vocab_size,embedding_dim,max_length,lstm_units):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim = min(vocabulary_size,max_vocab_size),\n",
    "                    output_dim = embedding_dim,\n",
    "                    input_length = max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        \n",
    "        LSTM(lstm_units,\n",
    "               dropout = 0.2,\n",
    "               recurrent_dropout=0.2,\n",
    "               return_sequences=True),\n",
    "        \n",
    "        LSTM(lstm_units//2,\n",
    "               dropout = 0.2,\n",
    "               recurrent_dropout=0.2\n",
    "               ),\n",
    "        Dense(16,activation='relu'),\n",
    "        Dense(1,activation='sigmoid')    \n",
    "    ])\n",
    "    model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    tf.keras.backend.clear_session()\n",
    "    model.build(input_shape=(None, max_length))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training utilities\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "def get_callbacks():\n",
    "    early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta = 0, # minimum change in the monitored quality to qualify as improvement\n",
    "    patience = 40, # number of epochs with no improvements after which training will be stopped\n",
    "    verbose = 1, #1 displays the message when the callback takes an action\n",
    "    mode = 'auto',\n",
    "    #One of {\"auto\", \"min\", \"max\"}.\n",
    "    # In min mode, training will stop when the quantity monitored has stopped decreasing;\n",
    "    # in \"max\" mode it will stop when the quantity monitored has stopped increasing;\n",
    "    #in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity. Defaults to \"auto\".\n",
    "    restore_best_weights =True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor = 0.2, #factor by which the learning rate will be reduced lr*factor\n",
    "    patience =10, #num,ber of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose = 1, # for updaing the messages\n",
    "    mode = 'auto',\n",
    "    min_lr = 0.001 #lowerbound for learning rate    \n",
    ")\n",
    "    callbacks = [early_stopping,reduce_lr]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc12e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def get_class_weights(y):\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    return dict(enumerate(class_weights))\n",
    "\n",
    "def apply_smote(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    return X_res, y_res\n",
    "def train_model(\n",
    "    model, X_train, y_train, X_val, y_val, callbacks,\n",
    "    batch_size=32, epochs=50, use_smote=False, use_class_weight=False\n",
    "):\n",
    "    \n",
    "    if use_smote:\n",
    "        X_train, y_train = apply_smote(X_train, y_train)\n",
    "        class_weight = None  \n",
    "    elif use_class_weight:\n",
    "        class_weight = get_class_weights(y_train)\n",
    "    else:\n",
    "        class_weight = None\n",
    "        \n",
    "    history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    class_weight = class_weight\n",
    ")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ba381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "\n",
    "def evaluate_model(model,X_test,y_test,threshold):\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob>threshold).astype(int).flatten()\n",
    "    print(\"\\nTest accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
    "    print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    df = load_and_clean('archive/SPAM text message 20170820 - Data.csv')\n",
    "    X, y, tokenizer, max_vocab_size = tokenize_and_pad(df)\n",
    "    max_length = X.shape[1]\n",
    "    vocab_size = min(len(tokenizer.word_index) + 1, max_vocab_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Build and train model\n",
    "    model = build_lstm_model_1_layer(vocab_size, max_length)\n",
    "    callbacks = get_callbacks()\n",
    "    history = train_model(model, X_train, y_train, X_test, y_test, callbacks, class_weight=class_weight_dict)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
